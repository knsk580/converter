{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install requests\n",
    "!pip install langchain\n",
    "!pip install langchain-text-splitters\n",
    "!pip install tiktoken\n",
    "!pip install docling\n",
    "\n",
    "!mkdir input\n",
    "!mkdir output\n",
    "!mkdir in_progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "input_dir = Path('/content/input')\n",
    "output_dir = Path('/content/output')\n",
    "in_progress_dir = Path('/content/in_progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"inputディレクトリ内のすべてのHTMLファイルを読み込む\"\"\"\n",
    "\n",
    "input_html_files = list(input_dir.glob('*.html')) + list(input_dir.glob('*.htm'))\n",
    "input_html_contents = {}\n",
    "for input_html_file in input_html_files:\n",
    "    print(f\"processing... {input_html_file.name}\")\n",
    "    # HTMLファイルを読み込み\n",
    "    with open(input_html_file, 'r', encoding='utf-8') as f:\n",
    "        input_html_contents[input_html_file.name] = f.read()\n",
    "        # ファイルサイズをKB単位でログ出力\n",
    "        file_size = input_html_file.stat().st_size / 1024\n",
    "        print(f\"{input_html_file.name} : {file_size:.1f} kb\")\n",
    "\n",
    "print(f\"html files : {len(input_html_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HTMLファイルのコンテンツによっては除外する\"\"\"\n",
    "\n",
    "# 除外パターン（後日設定予定。今は例として2つのパターンを仮置き）\n",
    "exclude_patterns = [\n",
    "    \"これは除外パターンの例1\",  # 例: \"広告\"\n",
    "    \"これは除外パターンの例2\",  # 例: \"サンプルテキスト\"\n",
    "]\n",
    "\n",
    "filtered_html_contents = {}\n",
    "excluded_files = []\n",
    "\n",
    "for filename, html_content in input_html_contents.items():\n",
    "    excluded = False\n",
    "    for pattern in exclude_patterns:\n",
    "        if pattern in html_content:\n",
    "            excluded = True\n",
    "            print(f\"{filename} is excluded for [{pattern}]\")\n",
    "            break\n",
    "    if excluded:\n",
    "        excluded_files.append(filename)\n",
    "    else:\n",
    "        filtered_html_contents[filename] = html_content\n",
    "\n",
    "print(f\"excluded files : {len(excluded_files)}\")\n",
    "print(f\"not excluded files : {len(filtered_html_contents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SourcePageURLを抽出する（後で使う）\"\"\"\n",
    "\n",
    "def extract_canonical_url(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # まずcanonicalを探す\n",
    "    canonical = soup.find('link', rel='canonical')\n",
    "    if canonical and canonical.get('href'):\n",
    "        return canonical['href']\n",
    "    # canonicalがなければog:urlを探す\n",
    "    og_url = soup.find('meta', property='og:url')\n",
    "    if og_url and og_url.get('content'):\n",
    "        return og_url['content']\n",
    "    return \"\"\n",
    "\n",
    "# 各HTMLファイルからcanonicalURLを抽出\n",
    "source_page_urls = {}\n",
    "canonical_found_count = 0\n",
    "canonical_not_found_count = 0\n",
    "\n",
    "for filename, html_content in filtered_html_contents.items():\n",
    "    print(f\"processing... {filename}\")\n",
    "    canonical_url = extract_canonical_url(html_content)\n",
    "    source_page_urls[filename] = canonical_url\n",
    "    \n",
    "    if canonical_url:\n",
    "        print(f\"{filename} : {canonical_url}\")\n",
    "        canonical_found_count += 1\n",
    "    else:\n",
    "        print(f\"{filename} : No canonical_url\")\n",
    "        canonical_not_found_count += 1\n",
    "\n",
    "print(f\"SourcePageURL extracted : total = {len(source_page_urls)}, founded = {canonical_found_count}, missing = {canonical_not_found_count}\")\n",
    "\n",
    "# デバッグしやすいようにファイルに出力しておく\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "in_progress_json_path = in_progress_dir / f\"source_page_urls_{timestamp}.json\"\n",
    "# source_page_urlsをoutputフォルダに保存\n",
    "with open(in_progress_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(source_page_urls, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"SourcePageURLs are saved as {in_progress_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"メジャーノイズを除去する\"\"\"\n",
    "major_noise_removed_html_contents = {}\n",
    "\n",
    "major_noise_tags = ['script', 'style', 'noscript', 'iframe', 'object', 'embed']\n",
    "\n",
    "for filename, html_content in filtered_html_contents.items():\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 基礎ノイズタグを削除\n",
    "    for tag in major_noise_tags:\n",
    "        for element in soup.find_all(tag):\n",
    "            element.decompose()\n",
    "    # コメントを削除\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "    # 基礎ノイズ除去後のHTMLを保存\n",
    "    major_noise_removed_html_contents[filename] = str(soup)\n",
    "\n",
    "# デバッグしやすいようにファイルに出力しておく\n",
    "major_noise_removed_dir = in_progress_dir / \"major_noise_removed\"\n",
    "major_noise_removed_dir.mkdir(parents=True, exist_ok=True)\n",
    "# 各HTMLコンテンツをmajor_noise_removedフォルダにファイルとして保存\n",
    "for filename, major_noise_removed_html in major_noise_removed_html_contents.items():\n",
    "    output_path = major_noise_removed_dir / filename\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(major_noise_removed_html)\n",
    "    print(f\"{filename} is saved as {output_path}\")\n",
    "\n",
    "print(f\"completed : {len(major_noise_removed_html_contents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"マイナーノイズを除去する\"\"\"\n",
    "\n",
    "# minor_noise_pattern.txtからパターンを読み込む\n",
    "minor_noise_patterns = []\n",
    "minor_noise_pattern_file = Path(\"minor_noise_pattern.txt\")\n",
    "with open(minor_noise_pattern_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    minor_noise_patterns = [line.strip() for line in f if line.strip()]\n",
    "print(f\"minor_noise_pattern.txtから{len(minor_noise_patterns)}件のパターンを読み込みました\")\n",
    "\n",
    "minor_noise_removed_html_contents = {}\n",
    "\n",
    "for filename, html_content in major_noise_removed_html_contents.items():\n",
    "    processed_content = html_content\n",
    "    for pattern in minor_noise_patterns:\n",
    "        if re.search(pattern, processed_content, flags=re.MULTILINE | re.DOTALL):\n",
    "            print(f\"ファイル: {filename} にパターン: {pattern} がヒットしました\")\n",
    "        processed_content = re.sub(pattern, '', processed_content, flags=re.MULTILINE | re.DOTALL)\n",
    "    minor_noise_removed_html_contents[filename] = processed_content\n",
    "\n",
    "# デバッグしやすいようにファイルに出力しておく\n",
    "minor_noise_removed_dir = in_progress_dir / \"minor_noise_removed\"\n",
    "minor_noise_removed_dir.mkdir(parents=True, exist_ok=True)\n",
    "# 各HTMLコンテンツをminor_noise_removedフォルダにファイルとして保存\n",
    "for filename, minor_noise_removed_html in minor_noise_removed_html_contents.items():\n",
    "    output_path = minor_noise_removed_dir / filename\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(minor_noise_removed_html)\n",
    "    print(f\"{filename} is saved as {output_path}\")\n",
    "\n",
    "print(f\"completed : {len(minor_noise_removed_html_contents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ノイズ除去済みのHTMLをMarkdownに変換する（doclingを使用）\"\"\"\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "markdown_contents = {}\n",
    "# in_progressフォルダ内にmarkdownフォルダを作成\n",
    "markdown_dir = in_progress_dir / \"markdown\"\n",
    "markdown_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename, html_content in minor_noise_removed_html_contents.items():    \n",
    "    # DocumentConverterを使用してHTMLファイルをMarkdownに変換\n",
    "    document_converter = DocumentConverter()\n",
    "    result = document_converter.convert_string(content=html_content, format=InputFormat.HTML, name=filename)\n",
    "    markdown = result.document.export_to_markdown()\n",
    "    \n",
    "    print(f\"{filename} のHTMLをMarkdownに変換しました\")\n",
    "    markdown_contents[filename] = markdown\n",
    "\n",
    "    # デバッグしやすいようにファイルに出力しておく\n",
    "    # 各Markdownコンテンツをmarkdownフォルダにファイルとして保存\n",
    "    output_md_path = markdown_dir / (Path(filename).stem + \".md\")\n",
    "    with open(output_md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown)\n",
    "    print(f\"{filename} is saved as {output_md_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Markdownをセクション分割する\"\"\"\n",
    "\n",
    "splitted_jsons = []\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4.1\")\n",
    "\n",
    "for filename, markdown_content in markdown_contents.items():\n",
    "    # MarkdownHeaderTextSplitterのインスタンスを作成\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header1\"),\n",
    "        (\"##\", \"Header2\"),\n",
    "        (\"###\", \"Header3\"),\n",
    "        (\"####\", \"Header4\"),\n",
    "    ]\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=True,\n",
    "        return_each_line=False,\n",
    "    )\n",
    "    splitted_docs = splitter.split_text(markdown_content)\n",
    "    print(f\"{filename} を {len(splitted_docs)} セクションに分割しました\")\n",
    "\n",
    "    # source_page_urlsからfilenameをkeyにしてsource_page_urlを取得し、metadataにセット\n",
    "    url = source_page_urls.get(filename, \"\")\n",
    "    # JSON形式に変換\n",
    "    for i, doc in enumerate(splitted_docs):\n",
    "        splitted_json = {\n",
    "            \"id\": f\"{filename}_{i}\",\n",
    "            \"content\": doc.page_content.strip(),\n",
    "            \"metadata\": {\n",
    "                \"source\": filename,\n",
    "                \"url\": url,\n",
    "                \"section_id\": i,\n",
    "                **doc.metadata\n",
    "            }\n",
    "        }\n",
    "        tokens = encoder.encode(json.dumps(splitted_json, ensure_ascii=False))\n",
    "        print(f\"{filename}_{i} : ({len(tokens)} tokens)\")\n",
    "        splitted_jsons.append(splitted_json)\n",
    "\n",
    "# in_progress_dir配下にjsonディレクトリを作成\n",
    "json_dir = in_progress_dir / \"json\"\n",
    "json_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_json_path = json_dir / (Path(filename).stem + \".json\")\n",
    "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(splitted_jsons, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
