{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y pandoc\n",
    "\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install requests\n",
    "!pip install langchain\n",
    "!pip install langchain-text-splitters\n",
    "\n",
    "!mkdir input\n",
    "!mkdir output\n",
    "!mkdir in_progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "import pypandoc\n",
    "\n",
    "\"\"\"ログ出力の準備\"\"\"\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "input_dir = Path('/content/input')\n",
    "output_dir = Path('/content/output')\n",
    "in_progress_dir = Path('/content/in_progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"inputディレクトリ内のすべてのHTMLファイルを読み込む\"\"\"\n",
    "input_html_files = list(input_dir.glob('*.html')) + list(input_dir.glob('*.htm'))\n",
    "input_html_contents = {}\n",
    "for input_html_file in input_html_files:\n",
    "    logger.info(f\"読み込み中 {input_html_file.name}\")\n",
    "    # HTMLファイルを読み込み\n",
    "    with open(input_html_file, 'r', encoding='utf-8') as f:\n",
    "        input_html_contents[input_html_file.name] = f.read()\n",
    "        # ファイルサイズをKB単位でログ出力\n",
    "        file_size = input_html_file.stat().st_size / 1024\n",
    "        logger.info(f\"{input_html_file.name} のファイルサイズ: {file_size:.1f} KB\")\n",
    "\n",
    "logger.info(f\"読み込んだHTMLファイル数: {len(input_html_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HTMLファイルの除外処理\"\"\"\n",
    "# 除外パターン（後日設定予定。今は例として2つのパターンを仮置き）\n",
    "exclude_patterns = [\n",
    "    \"これは除外パターンの例1\",  # 例: \"広告\"\n",
    "    \"これは除外パターンの例2\",  # 例: \"サンプルテキスト\"\n",
    "]\n",
    "\n",
    "filtered_html_contents = {}\n",
    "excluded_files = []\n",
    "\n",
    "for filename, html_content in input_html_contents.items():\n",
    "    excluded = False\n",
    "    for pattern in exclude_patterns:\n",
    "        if pattern in html_content:\n",
    "            excluded = True\n",
    "            logger.info(f\"除外された({pattern}) : {filename}\")\n",
    "            break\n",
    "    if excluded:\n",
    "        excluded_files.append(filename)\n",
    "    else:\n",
    "        filtered_html_contents[filename] = html_content\n",
    "\n",
    "logger.info(f\"除外されたファイル数: {len(excluded_files)} 件\")\n",
    "logger.info(f\"除外されなかったファイル数: {len(filtered_html_contents)} 件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SourcePageURLを抽出\"\"\"\n",
    "def extract_canonical_url(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    canonical = soup.find('link', rel='canonical')\n",
    "    if canonical and canonical.get('href'):\n",
    "        return canonical['href']\n",
    "    return \"\"\n",
    "\n",
    "# 各HTMLファイルからcanonicalURLを抽出\n",
    "source_page_urls = {}\n",
    "canonical_found_count = 0\n",
    "canonical_not_found_count = 0\n",
    "\n",
    "for filename, html_content in filtered_html_contents.items():\n",
    "    logger.info(f\"SourcePage抽出中: {filename}\")\n",
    "    canonical_url = extract_canonical_url(html_content)\n",
    "    source_page_urls[filename] = canonical_url\n",
    "    \n",
    "    if canonical_url:\n",
    "        logger.info(f\"{filename} -> {canonical_url}\")\n",
    "        canonical_found_count += 1\n",
    "    else:\n",
    "        logger.warning(f\"{filename} -> No canonical_url\")\n",
    "        canonical_not_found_count += 1\n",
    "\n",
    "logger.info(f\"SourcePageURL 抽出処理完了: {len(source_page_urls)} 件, 発見できた: {canonical_found_count} 件, 発見できなかった: {canonical_not_found_count} 件\")\n",
    "\n",
    "# タイムスタンプを生成（例: 20240608153045）\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "in_progress_json_path = in_progress_dir / f\"source_page_urls_{timestamp}.json\"\n",
    "# source_page_urlsをoutputフォルダに保存\n",
    "with open(in_progress_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(source_page_urls, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"source_page_urlsを {in_progress_json_path} に出力しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"基礎ノイズを除去\"\"\"\n",
    "major_noise_removed_html_contents = {}\n",
    "\n",
    "major_noise_tags = ['script', 'style', 'noscript', 'iframe', 'object', 'embed']\n",
    "\n",
    "for filename, html_content in filtered_html_contents.items():\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 基礎ノイズタグを削除\n",
    "    for tag in major_noise_tags:\n",
    "        for element in soup.find_all(tag):\n",
    "            element.decompose()\n",
    "    # コメントを削除\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "    # 基礎ノイズ除去後のHTMLを保存\n",
    "    major_noise_removed_html_contents[filename] = str(soup)\n",
    "\n",
    "# in_progressフォルダ内にmajor_noise_removedフォルダを作成\n",
    "major_noise_removed_dir = in_progress_dir / \"major_noise_removed\"\n",
    "major_noise_removed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 各HTMLコンテンツをmajor_noise_removedフォルダにファイルとして保存\n",
    "for filename, major_noise_removed_html in major_noise_removed_html_contents.items():\n",
    "    output_path = major_noise_removed_dir / filename\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(major_noise_removed_html)\n",
    "    logger.info(f\"{filename} を {output_path} に保存しました\")\n",
    "\n",
    "logger.info(f\"基礎ノイズ除去済みHTMLを {len(major_noise_removed_html_contents)} 件保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"末梢ノイズを除去\"\"\"\n",
    "\n",
    "# minor_noise_pattern.txtからパターンを読み込む\n",
    "minor_noise_patterns = []\n",
    "minor_noise_pattern_file = Path(\"minor_noise_pattern.txt\")\n",
    "with open(minor_noise_pattern_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    minor_noise_patterns = [line.strip() for line in f if line.strip()]\n",
    "logger.info(f\"minor_noise_pattern.txtから{len(minor_noise_patterns)}件のパターンを読み込みました\")\n",
    "\n",
    "minor_noise_removed_html_contents = {}\n",
    "\n",
    "for filename, html_content in major_noise_removed_html_contents.items():\n",
    "    processed_content = html_content\n",
    "    for pattern in minor_noise_patterns:\n",
    "        if re.search(pattern, processed_content, flags=re.MULTILINE | re.DOTALL):\n",
    "            logger.info(f\"ファイル: {filename} にパターン: {pattern} がヒットしました\")\n",
    "        processed_content = re.sub(pattern, '', processed_content, flags=re.MULTILINE | re.DOTALL)\n",
    "    minor_noise_removed_html_contents[filename] = processed_content\n",
    "\n",
    "# in_progressフォルダ内にminor_noise_removedフォルダを作成\n",
    "minor_noise_removed_dir = in_progress_dir / \"minor_noise_removed\"\n",
    "minor_noise_removed_dir.mkdir(parents=True, exist_ok=True)\n",
    "# 各HTMLコンテンツをminor_noise_removedフォルダにファイルとして保存\n",
    "for filename, minor_noise_removed_html in minor_noise_removed_html_contents.items():\n",
    "    output_path = minor_noise_removed_dir / filename\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(minor_noise_removed_html)\n",
    "    logger.info(f\"{filename} を {output_path} に保存しました\")\n",
    "\n",
    "logger.info(f\"末梢ノイズ除去済みHTMLを {len(minor_noise_removed_html_contents)} 件保存しました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Markdownに変換\"\"\"\n",
    "markdown_contents = {}\n",
    "for filename, html_content in minor_noise_removed_html_contents.items():\n",
    "    markdown = pypandoc.convert_text(\n",
    "        html_content, \n",
    "        'commonmark',\n",
    "        format='html',\n",
    "        extra_args=['--wrap=none']\n",
    "    )\n",
    "    logger.info(f\"{filename} のHTMLをMarkdownに変換しました\")\n",
    "    markdown_contents[filename] = markdown\n",
    "    # in_progressフォルダ内にmarkdownフォルダを作成\n",
    "    markdown_dir = in_progress_dir / \"markdown\"\n",
    "    markdown_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 各Markdownコンテンツをmarkdownフォルダにファイルとして保存\n",
    "    output_md_path = markdown_dir / (Path(filename).stem + \".md\")\n",
    "    with open(output_md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown)\n",
    "    logger.info(f\"{filename} を {output_md_path} に保存しました\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_contents = {}\n",
    "\n",
    "for filename, markdown_content in markdown_contents.items():\n",
    "    # MarkdownHeaderTextSplitterのインスタンスを作成\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\"),\n",
    "    ]\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    docs = splitter.split_text(markdown_content)\n",
    "    json_contents[filename] = docs\n",
    "    logger.info(f\"{filename} を {len(docs)} セクションに分割し、JSON化しました\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
